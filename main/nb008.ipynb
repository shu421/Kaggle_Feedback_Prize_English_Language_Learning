{"cells":[{"cell_type":"markdown","metadata":{"id":"yHHiMfFSkk5Y"},"source":["# Overview\n","This notebook is for training Longformer-base-4096 of 5 folds and calculating cv score after postprocessing predictions by using average probability of predicted classes. \n","\n","A notebook for inferencing is below.\n","\n","https://www.kaggle.com/ytakayama/inference-pytorch-longformer-5fold\n","\n","# Reference\n","Following notebooks are very informative and great. Thanks.\n","- https://www.kaggle.com/abhishek/two-longformers-are-better-than-1\n","- https://www.kaggle.com/cdeotte/pytorch-bigbird-ner-cv-0-615\n","- https://www.kaggle.com/nbroad/corrected-train-csv-feedback-prize\n","\n","\n","# How to infer\n","- calculate probability for 15 classes by 5 fold model(LongFormer)\n","\n","15 classes: OUTPUT_LABELS in \"constants\" header which mean 14 combinations of 2 NER tags(B- /I-) and 7 elements + others\n","\n","- calculate class of the highest probability \n"," - inference test data: calculate average probability in 5 predictions\n"," - validate train data: use probability of each fold\n","- postprocess based on probability of predicted class and how long predcited class is continuous\n","\n","# customize for executing this notebook on Google Colab\n","- prepare data: competion data and following data\n","\n","https://www.kaggle.com/nbroad/corrected-train-csv-feedback-prize\n","- set variables about directories of Config class (e.g. data_dir)"]},{"cell_type":"markdown","metadata":{"id":"7bDVKrA728Vn"},"source":["## setup envirionment"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":170454,"status":"ok","timestamp":1644331730003,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"3zuiIZsfklQp","outputId":"09ffa272-9349-4080-b4ce-3f1889deb3aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Kaggle_Feedback-Prize-Evaluating-Student-Writing/main\n","\u001b[K     |████████████████████████████████| 1.2 MB 12.2 MB/s \n","\u001b[K     |████████████████████████████████| 3.5 MB 49.5 MB/s \n","\u001b[K     |████████████████████████████████| 831.4 MB 6.0 kB/s \n","\u001b[K     |████████████████████████████████| 22.1 MB 321 kB/s \n","\u001b[K     |████████████████████████████████| 1.9 MB 54.8 MB/s \n","\u001b[K     |████████████████████████████████| 7.6 MB 15.0 MB/s \n","\u001b[K     |████████████████████████████████| 4.1 MB 52.0 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 64.3 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 60.4 MB/s \n","\u001b[K     |████████████████████████████████| 6.8 MB 51.0 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n","\u001b[?25h  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["import os\n","\n","if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\") is None:\n","    ON_KAGGLE = False\n","else:\n","    ON_KAGGLE = True\n","if not ON_KAGGLE:\n","    import shutil\n","    from requests import get\n","\n","    from google.colab import drive, files\n","    # mount Google Drive\n","    drive.mount(\"/content/drive\")\n","    %cd drive/MyDrive/Kaggle_Feedback-Prize-Evaluating-Student-Writing/main/\n","    !pip install  -qq sentencepiece transformers torch==1.9.1 torchvision==0.10.1 torchAudio==0.9.1 torchtext==0.10.1\n","else:\n","    for dirname, _, filenames in os.walk('/kaggle/input'):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))"]},{"cell_type":"markdown","metadata":{"id":"Ip73rtLOC8tV"},"source":["Config"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1644331730004,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"KgOMqia7C8Rn"},"outputs":[],"source":["class Config:\n","    name = 'fb_nb008'\n","    model_savename = 'longformer'\n","    if ON_KAGGLE:\n","        model_name = '../input/pt-longformer-base' # https://www.kaggle.com/kishalmandal/pt-longformer-base\n","        # base_dir = '/content/drive/MyDrive/petfinder'\n","        data_dir = '../input/feedback-prize-2021/'\n","        pre_data_dir = './preprocessed/'\n","        model_dir = '.'\n","        output_dir = '.'\n","    else:\n","        # customize for my own Google Colab Environment\n","        model_name = 'SpanBERT/spanbert-large-cased'\n","        # model_name = 'allenai/longformer-base-4096' # download from Internet\n","        base_dir = '/content/drive/MyDrive/Kaggle_Feedback-Prize-Evaluating-Student-Writing/'\n","        data_dir = os.path.join(base_dir, 'input/feedback-prize-2021/')\n","        pre_data_dir = os.path.join(base_dir, 'data/preprocessed')\n","        model_dir = os.path.join(base_dir, f'model/{name}')\n","        output_dir = os.path.join(base_dir, f'output/{name}')\n","    is_debug = False\n","    load_texts = True\n","    n_epoch = 3 # not to exceed runtime limits on Kaggle\n","    n_fold = 5\n","    verbose_steps = 500\n","    random_seed = 71\n","    max_length = 1024\n","    train_batch_size = 8\n","    valid_batch_size = 8\n","    lr = 5e-5\n","    num_labels = 15\n","    label_subtokens = True\n","    output_hidden_states = True\n","    hidden_dropout_prob = 0.1\n","    layer_norm_eps = 1e-7\n","    add_pooling_layer = False\n","    verbose_steps = 500\n","    stride = 128\n","    if is_debug:\n","        debug_sample = 1000\n","        verbose_steps = 16\n","        n_epoch = 1\n","        n_fold = 2"]},{"cell_type":"markdown","metadata":{"id":"iP34ixYneANA"},"source":["constants"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644331730005,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"e0IRAsjceDS2"},"outputs":[],"source":["IGNORE_INDEX = -100\n","NON_LABEL = -1\n","OUTPUT_LABELS = ['0', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n","                 'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n","LABELS_TO_IDS = {v:k for k,v in enumerate(OUTPUT_LABELS)}\n","IDS_TO_LABELS = {k:v for k,v in enumerate(OUTPUT_LABELS)}\n","\n","MIN_THRESH = {\n","    \"I-Lead\": 9,\n","    \"I-Position\": 5,\n","    \"I-Evidence\": 14,\n","    \"I-Claim\": 3,\n","    \"I-Concluding Statement\": 11,\n","    \"I-Counterclaim\": 6,\n","    \"I-Rebuttal\": 4,\n","}\n","\n","PROB_THRESH = {\n","    \"I-Lead\": 0.7,\n","    \"I-Position\": 0.55,\n","    \"I-Evidence\": 0.65,\n","    \"I-Claim\": 0.55,\n","    \"I-Concluding Statement\": 0.7,\n","    \"I-Counterclaim\": 0.5,\n","    \"I-Rebuttal\": 0.55,\n","}"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1644331730005,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"ah8WKsqGHja0"},"outputs":[],"source":["if not ON_KAGGLE:\n","    if not os.path.exists(Config.model_dir):\n","        os.makedirs(Config.model_dir, exist_ok=True)\n","    if not os.path.exists(Config.output_dir):\n","        os.makedirs(Config.output_dir, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"1MzNuNB1kvb3"},"source":["### libraries"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644331730006,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"JnVna5XqnuWN"},"outputs":[],"source":["# if not ON_KAGGLE:\n","#     !pip install -qq transformers"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2076,"status":"ok","timestamp":1644331732072,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"DZe70phDk1QF"},"outputs":[],"source":["# general\n","import pandas as pd\n","import numpy as np\n","import time\n","import japanize_matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","sns.set()\n","japanize_matplotlib.japanize()\n","import random\n","from tqdm.notebook import tqdm\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","import gc\n","from collections import defaultdict\n","# nlp\n","from sklearn.feature_extraction.text import CountVectorizer\n","import torch\n","import torch.nn as nn\n","from transformers import LongformerConfig, LongformerModel, LongformerTokenizerFast\n","from transformers import AutoModel, AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler"]},{"cell_type":"markdown","metadata":{"id":"OzvQtzK4ni9Q"},"source":["## preprocess\n","use corrected train.csv\n","\n","https://www.kaggle.com/nbroad/corrected-train-csv-feedback-prize/notebook"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2597,"status":"ok","timestamp":1644331734663,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"-wS1I6Xsmnla"},"outputs":[],"source":["if ON_KAGGLE:\n","    df_alltrain = pd.read_csv('../input/corrected-train-csv-feedback-prize/corrected_train.csv')\n","else:\n","    df_alltrain = pd.read_csv(f'{Config.data_dir}/corrected_train.csv')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1644331734664,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"wkADwF11m1nK"},"outputs":[],"source":["def agg_essays(train_flg):\n","    folder = 'train' if train_flg else 'test'\n","    names, texts =[], []\n","    for f in tqdm(list(os.listdir(f'{Config.data_dir}/{folder}'))):\n","        names.append(f.replace('.txt', ''))\n","        texts.append(open(f'{Config.data_dir}/{folder}/' + f, 'r').read())\n","        df_texts = pd.DataFrame({'id': names, 'text': texts})\n","\n","    df_texts['text_split'] = df_texts.text.str.split()\n","    print('Completed tokenizing texts.')\n","    return df_texts"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1644331734665,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"5pL88Q7UqItJ"},"outputs":[],"source":["def ner(df_texts, df_train):\n","    all_entities = []\n","    for _,  row in tqdm(df_texts.iterrows(), total=len(df_texts)):\n","        total = len(row['text_split'])\n","        entities = ['0'] * total\n","\n","        for _, row2 in df_train[df_train['id'] == row['id']].iterrows():\n","            discourse = row2['discourse_type']\n","            list_ix = [int(x) for x in row2['predictionstring'].split(' ')]\n","            entities[list_ix[0]] = f'B-{discourse}'\n","            for k in list_ix[1:]: entities[k] = f'I-{discourse}'\n","        all_entities.append(entities)\n","\n","    df_texts['entities'] = all_entities\n","    print('Completed mapping discourse to each token.')\n","    return df_texts"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3522,"status":"ok","timestamp":1644331738180,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"IZKDoHIhhZiO"},"outputs":[],"source":["if not Config.load_texts:    \n","    def preprocess(df_train = None):\n","        if df_train is None:\n","            train_flg = False\n","        else:\n","            train_flg = True\n","        \n","        df_texts = agg_essays(train_flg)\n","\n","        if train_flg:\n","            df_texts = ner(df_texts, df_train)\n","        return df_texts\n","    \n","    alltrain_texts = preprocess(df_alltrain)\n","    test_texts = preprocess()\n","    # alltrain_texts.to_pickle('../input/fb-data/alltrain_texts_correct.pkl')\n","    # test_texts.to_pickle('../input/fb-data/test_texts_correct.pkl')\n","else:\n","    alltrain_texts = pd.read_pickle('../input/fb-data/alltrain_texts_correct.pkl')\n","    test_texts = pd.read_pickle('../input/fb-data/test_texts_correct.pkl')"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1644331738180,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"yDSJobRdjdqs","outputId":"b2ea3522-a805-4c9d-eb24-2b6bba2c119f"},"outputs":[{"output_type":"stream","name":"stdout","text":["15594\n"]}],"source":["if Config.is_debug:\n","    alltrain_texts = alltrain_texts.sample(Config.debug_sample).reset_index(drop=True)\n","print(len(alltrain_texts))"]},{"cell_type":"markdown","metadata":{"id":"DKENi2ZOkJv1"},"source":["set seed & split train/test"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1644331738181,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"zrYLMWzfljQ0","outputId":"a6bf16b8-69dd-421b-e460-31d04188c26e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["def seed_everything(seed=Config.random_seed):\n","    #os.environ['PYTHONSEED'] = str(seed)\n","    np.random.seed(seed%(2**32-1))\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic=True\n","    torch.backends.cudnn.benchmark = False\n","\n","seed_everything()\n","# device optimization\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644331738181,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"zsZXHSH-vcGA","outputId":"09ae6a28-7011-4bc8-cb7d-7583f23b0d9e"},"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-685b8cef-a261-4ac5-b62c-1aeeb55e35e0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>text_split</th>\n","      <th>entities</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>F48EF80D2ED3</td>\n","      <td>There are many programs in the world around yo...</td>\n","      <td>[There, are, many, programs, in, the, world, a...</td>\n","      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>F8FB4470A52F</td>\n","      <td>Dear Senator,\\n\\n\"The Electoral College is a p...</td>\n","      <td>[Dear, Senator,, \"The, Electoral, College, is,...</td>\n","      <td>[0, 0, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>F176A8CF72BB</td>\n","      <td>In my opinion i don't think that is fair. i th...</td>\n","      <td>[In, my, opinion, i, don't, think, that, is, f...</td>\n","      <td>[B-Position, I-Position, I-Position, I-Positio...</td>\n","      <td>4.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>EBDE7FC748A4</td>\n","      <td>Unmasking the Face\\n\\nThe face on Mars was rea...</td>\n","      <td>[Unmasking, the, Face, The, face, on, Mars, wa...</td>\n","      <td>[0, 0, 0, B-Position, I-Position, I-Position, ...</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>F6C40C564E5E</td>\n","      <td>Luke think you should join the seagoing cowboy...</td>\n","      <td>[Luke, think, you, should, join, the, seagoing...</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, B-Claim, I-Clai...</td>\n","      <td>4.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-685b8cef-a261-4ac5-b62c-1aeeb55e35e0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-685b8cef-a261-4ac5-b62c-1aeeb55e35e0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-685b8cef-a261-4ac5-b62c-1aeeb55e35e0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["             id  ... fold\n","0  F48EF80D2ED3  ...  0.0\n","1  F8FB4470A52F  ...  0.0\n","2  F176A8CF72BB  ...  4.0\n","3  EBDE7FC748A4  ...  3.0\n","4  F6C40C564E5E  ...  4.0\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":13}],"source":["def split_fold(df_train):\n","    ids = df_train['id'].unique()\n","    kf = KFold(n_splits=Config.n_fold, shuffle = True, random_state=Config.random_seed)\n","    for i_fold, (_, valid_index) in enumerate(kf.split(ids)):\n","        df_train.loc[valid_index,'fold'] = i_fold\n","    return df_train\n","\n","alltrain_texts = split_fold(alltrain_texts)\n","alltrain_texts.head()"]},{"cell_type":"markdown","metadata":{"id":"NjsguQtHonKZ"},"source":["## dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644331738938,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"-eRhZHHNprqv"},"outputs":[],"source":["class FeedbackPrizeDataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len, has_labels):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.has_labels = has_labels\n","    \n","    def __getitem__(self, index):\n","        text = self.data['text'][index]\n","        encoding = self.tokenizer(\n","            text.split(),\n","            is_split_into_words = True,\n","            padding = 'max_length',\n","            truncation = True,\n","            max_length = self.max_len,\n","            # stride=Config.stride\n","        )\n","        word_ids = encoding.word_ids()\n","\n","        # targets\n","        if self.has_labels:\n","            word_labels = self.data.entities[index]\n","            prev_word_idx = None\n","            labels_ids = []\n","            for word_idx in word_ids:\n","                if word_idx is None:\n","                    labels_ids.append(IGNORE_INDEX)\n","                elif word_idx != prev_word_idx:\n","                    labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n","                else:\n","                    if Config.label_subtokens:\n","                        labels_ids.append(LABELS_TO_IDS[word_labels[word_idx]])\n","                    else:\n","                        labels_ids.append(IGNORE_INDEX)\n","                prev_word_idx = word_idx\n","            encoding['labels'] = labels_ids\n","        # convert to torch.tensor\n","        item = {k: torch.as_tensor(v) for k, v in encoding.items()}\n","        word_ids2 = [w if w is not None else NON_LABEL for w in word_ids]\n","        item['word_ids'] = torch.as_tensor(word_ids2)\n","        return item\n","\n","    def __len__(self):\n","        return self.len"]},{"cell_type":"markdown","metadata":{"id":"LoKi2f7lC2UR"},"source":["## model"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644331738939,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"qlfQN-BpC3Et"},"outputs":[],"source":["class FeedbackModel(nn.Module):\n","    def __init__(self):\n","        super(FeedbackModel, self).__init__()\n","        model_config = LongformerConfig.from_pretrained(Config.model_name)\n","        self.model_config = model_config\n","        self.model = LongformerModel.from_pretrained(Config.model_name, config=model_config)\n","        # model_config = AutoConfig.from_pretrained(Config.model_name)\n","        # self.modle_config = model_config\n","        # self.model = AutoModelForTokenClassification.from_pretrained(Config.model_name, config=model_config)\n","        self.head = nn.Linear(model_config.hidden_size, Config.num_labels)\n","    \n","    def forward(self, input_ids, mask):\n","        x = self.model(input_ids, mask)\n","        logits = self.head(x[0])\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"tIzPUw0n21Cq"},"source":["## utility function"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644331738939,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"CHZI4rsi2yO-"},"outputs":[],"source":["def active_logits(raw_logits, word_ids):\n","    word_ids = word_ids.view(-1)\n","    active_mask = word_ids.unsqueeze(1).expand(word_ids.shape[0], Config.num_labels)\n","    active_mask = active_mask != NON_LABEL\n","    active_logits = raw_logits.view(-1, Config.num_labels)\n","    active_logits = torch.masked_select(active_logits, active_mask) # return 1dTensor\n","    active_logits = active_logits.view(-1, Config.num_labels) \n","    return active_logits\n","\n","def active_labels(labels):\n","    active_mask = labels.view(-1) != IGNORE_INDEX\n","    active_labels = torch.masked_select(labels.view(-1), active_mask)\n","    return active_labels\n","\n","def active_preds_prob(active_logits):\n","    active_preds = torch.argmax(active_logits, axis = 1)\n","    active_preds_prob, _ = torch.max(active_logits, axis = 1)\n","    return active_preds, active_preds_prob"]},{"cell_type":"markdown","metadata":{"id":"6tJJuIK01jGT"},"source":["## evaluating function"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1644331738939,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"NGIRUiBf1rkM"},"outputs":[],"source":["def calc_overlap(row):\n","    \"\"\"\n","    calculate the overlap between prediction and ground truth\n","    \"\"\"\n","    set_pred = set(row.new_predictionstring_pred.split(' '))\n","    set_gt = set(row.new_predictionstring_gt.split(' '))\n","    # length of each end intersection\n","    len_pred = len(set_pred)\n","    len_gt = len(set_gt)\n","    intersection = len(set_gt.intersection(set_pred))\n","    overlap_1 = intersection / len_gt\n","    overlap_2 = intersection / len_pred\n","    return [overlap_1, overlap_2]\n","\n","def score_feedback_comp(pred_df, gt_df):\n","    \"\"\"\n","    A function that scores for the kaggle\n","        Student Writing Competition\n","        \n","    Uses the steps in the evaluation page here:\n","        https://www.kaggle.com/c/feedback-prize-2021/overview/evaluation\n","    \"\"\"\n","    gt_df = gt_df[['id', 'discourse_type', 'new_predictionstring']].reset_index(drop = True).copy()\n","    pred_df = pred_df[['id', 'class', 'new_predictionstring']].reset_index(drop = True).copy()\n","    gt_df['gt_id'] = gt_df.index\n","    pred_df['pred_id'] = pred_df.index\n","    joined = pred_df.merge(\n","        gt_df,\n","        left_on = ['id', 'class'],\n","        right_on = ['id', 'discourse_type'],\n","        how = 'outer',\n","        suffixes = ['_pred', '_gt']\n","    )\n","    joined['new_predictionstring_gt'] =  joined['new_predictionstring_gt'].fillna(' ')\n","    joined['new_predictionstring_pred'] =  joined['new_predictionstring_pred'].fillna(' ')\n","    joined['overlaps'] = joined.apply(calc_overlap, axis = 1)\n","    # overlap over 0.5: true positive\n","    # If nultiple overlaps exists, the higher is taken.\n","    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n","    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n","\n","    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n","    joined['max_overlap'] = joined[['overlap1', 'overlap2']].max(axis = 1)\n","    tp_pred_ids = joined.query('potential_TP').sort_values('max_overlap', ascending = False)\\\n","                  .groupby(['id', 'new_predictionstring_gt']).first()['pred_id'].values\n","    \n","    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n","    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n","    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n","\n","    TP = len(tp_pred_ids)\n","    FP = len(fp_pred_ids)\n","    FN = len(unmatched_gt_ids)\n","    macro_f1_score = TP / (TP + 1/2 * (FP + FN))\n","    return macro_f1_score\n","\n","def oof_score(df_val, oof):\n","    f1score = []\n","    classes = ['Lead', 'Position','Claim', 'Counterclaim', 'Rebuttal','Evidence','Concluding Statement']\n","    for c in classes:\n","        pred_df = oof.loc[oof['class'] == c].copy()\n","        gt_df = df_val.loc[df_val['discourse_type'] == c].copy()\n","        f1 = score_feedback_comp(pred_df, gt_df)\n","        print(f'{c:<10}: {f1:4f}')\n","        f1score.append(f1)\n","    f1avg = np.mean(f1score)\n","    return f1avg"]},{"cell_type":"markdown","metadata":{"id":"a6NYa6Op2Cwu"},"source":["## inferencing function"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644331738940,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"9wKfL0KO28Vy"},"outputs":[],"source":["def inference(model, dl, criterion, valid_flg):\n","    final_predictions = []\n","    final_predictions_prob = []\n","    stream = tqdm(dl)\n","    model.eval()\n","    \n","    valid_loss = 0\n","    valid_accuracy = 0\n","    all_logits = None\n","    for batch_idx, batch in enumerate(stream, start = 1):\n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        with torch.no_grad():\n","            raw_logits = model(input_ids=ids, mask = mask)\n","        del ids, mask\n","        \n","        word_ids = batch['word_ids'].to(device, dtype = torch.long)\n","        if valid_flg:    \n","            raw_labels = batch['labels'].to(device, dtype = torch.long)\n","            logits = active_logits(raw_logits, word_ids)\n","            labels = active_labels(raw_labels)\n","            preds, preds_prob = active_preds_prob(logits)\n","            valid_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n","            loss = criterion(logits, labels)\n","            valid_loss += loss.item()\n","        \n","        if batch_idx == 1:\n","            all_logits = raw_logits.cpu().numpy()\n","        else:\n","            all_logits = np.append(all_logits, raw_logits.cpu().numpy(), axis=0)\n","\n","    \n","    if valid_flg:        \n","        epoch_loss = valid_loss / batch_idx\n","        epoch_accuracy = valid_accuracy / batch_idx\n","    else:\n","        epoch_loss, epoch_accuracy = 0, 0\n","    return all_logits, epoch_loss, epoch_accuracy\n","\n","\n","def preds_class_prob(all_logits, dl):\n","    print(\"predict target class and its probabilty\")\n","    final_predictions = []\n","    final_predictions_score = []\n","    stream = tqdm(dl)\n","    len_sample = all_logits.shape[0]\n","\n","    for batch_idx, batch in enumerate(stream, start=0):\n","        for minibatch_idx in range(Config.valid_batch_size):\n","            sample_idx = int(batch_idx * Config.valid_batch_size + minibatch_idx)\n","            if sample_idx > len_sample - 1 : break\n","            word_ids = batch['word_ids'][minibatch_idx].numpy()\n","            predictions =[]\n","            predictions_prob = []\n","            pred_class_id = np.argmax(all_logits[sample_idx], axis=1)\n","            pred_score = np.max(all_logits[sample_idx], axis=1)\n","            pred_class_labels = [IDS_TO_LABELS[i] for i in pred_class_id]\n","            prev_word_idx = -1\n","            for idx, word_idx in enumerate(word_ids):\n","                if word_idx == -1:\n","                    pass\n","                elif word_idx != prev_word_idx:\n","                    predictions.append(pred_class_labels[idx])\n","                    predictions_prob.append(pred_score[idx])\n","                    prev_word_idx = word_idx\n","            final_predictions.append(predictions)\n","            final_predictions_score.append(predictions_prob)\n","    return final_predictions, final_predictions_score"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1644331738940,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"DwjxrC2WZYH8"},"outputs":[],"source":["def get_preds_onefold(model, df, dl, criterion, valid_flg):\n","    logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\n","    all_preds, all_preds_prob = preds_class_prob(logits, dl)\n","    df_pred = post_process_pred(df, all_preds, all_preds_prob)\n","    return df_pred, valid_loss, valid_acc\n","\n","def get_preds_folds(df, dl, criterion, valid_flg=False):\n","    for i_fold in range(Config.n_fold):\n","        model_filename = os.path.join(Config.model_dir, f\"{Config.model_savename}_{i_fold}.bin\")\n","        print(f\"{model_filename} inference\")\n","        model = FeedbackModel()\n","        model = model.to(device)\n","        model.load_state_dict(torch.load(model_filename))\n","        logits, valid_loss, valid_acc = inference(model, dl, criterion, valid_flg)\n","        if i_fold == 0:\n","            avg_pred_logits = logits\n","        else:\n","            avg_pred_logits += logits\n","    avg_pred_logits /= Config.n_fold\n","    all_preds, all_preds_prob = preds_class_prob(avg_pred_logits, dl)\n","    df_pred = post_process_pred(df, all_preds, all_preds_prob)\n","    return df_pred\n","\n","def post_process_pred(df, all_preds, all_preds_prob):\n","    final_preds = []\n","    for i in range(len(df)):\n","        idx = df.id.values[i]\n","        pred = all_preds[i]\n","        pred_prob = all_preds_prob[i]\n","        j = 0\n","        while j < len(pred):\n","            cls = pred[j]\n","            if cls == '0': j += 1\n","            else: cls = cls.replace('B', 'I')\n","            end = j + 1\n","            while end < len(pred) and pred[end] == cls:\n","                end += 1\n","            if cls != '0' and cls !='':\n","                avg_score = np.mean(pred_prob[j:end])\n","                if end - j > MIN_THRESH[cls] and avg_score > PROB_THRESH[cls]:\n","                    final_preds.append((idx, cls.replace('I-', ''), ' '.join(map(str, list(range(j, end))))))\n","            j = end\n","    df_pred = pd.DataFrame(final_preds)\n","    df_pred.columns = ['id', 'class', 'new_predictionstring']\n","    return df_pred"]},{"cell_type":"markdown","metadata":{"id":"hCw88fKN2bWF"},"source":["## training and validating function"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1644331738940,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"EJUnTD0w2aZJ"},"outputs":[],"source":["def train_fn(model, dl_train, optimizer, epoch, criterion):\n","    model.train()\n","    train_loss = 0\n","    train_accuracy = 0\n","    stream = tqdm(dl_train)\n","    scaler = GradScaler()\n","\n","    for batch_idx, batch in enumerate(stream, start = 1):\n","        ids = batch['input_ids'].to(device, dtype = torch.long)\n","        mask = batch['attention_mask'].to(device, dtype = torch.long)\n","        raw_labels = batch['labels'].to(device, dtype = torch.long)\n","        word_ids = batch['word_ids'].to(device, dtype = torch.long)\n","        optimizer.zero_grad()\n","        with autocast():\n","            print(ids.shape, mask.shape)\n","            raw_logits = model(input_ids = ids, mask = mask)\n","        \n","        logits = active_logits(raw_logits, word_ids)\n","        labels = active_labels(raw_labels)\n","        preds, preds_prob = active_preds_prob(logits)\n","        train_accuracy += accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n","        criterion = nn.CrossEntropyLoss()\n","        loss = criterion(logits, labels)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        train_loss += loss.item()\n","        \n","        if batch_idx % Config.verbose_steps == 0:\n","            loss_step = train_loss / batch_idx\n","            print(f'Training loss after {batch_idx:04d} training steps: {loss_step}')\n","            \n","    epoch_loss = train_loss / batch_idx\n","    epoch_accuracy = train_accuracy / batch_idx\n","    del dl_train, raw_logits, logits, raw_labels, preds, labels\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    print(f'epoch {epoch} - training loss: {epoch_loss:.4f}')\n","    print(f'epoch {epoch} - training accuracy: {epoch_accuracy:.4f}')\n","    return train_loss"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1644331738941,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"},"user_tz":-540},"id":"SUNaOnL5a5UM"},"outputs":[],"source":["def valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion):\n","    oof, valid_loss, valid_acc  = get_preds_onefold(model, df_val, dl_val, criterion, valid_flg=True)\n","    f1score =[]\n","    # classes = oof['class'].unique()\n","    classes = ['Lead', 'Position', 'Claim','Counterclaim', 'Rebuttal','Evidence','Concluding Statement']\n","    print(f\"Validation F1 scores\")\n","\n","    for c in classes:\n","        pred_df = oof.loc[oof['class'] == c].copy()\n","        gt_df = df_val_eval.loc[df_val_eval['discourse_type'] == c].copy()\n","        f1 = score_feedback_comp(pred_df, gt_df)\n","        print(f' * {c:<10}: {f1:4f}')\n","        f1score.append(f1)\n","    f1avg = np.mean(f1score)\n","    print(f'Overall Validation avg F1: {f1avg:.4f} val_loss:{valid_loss:.4f} val_accuracy:{valid_acc:.4f}')\n","    return valid_loss, oof"]},{"cell_type":"markdown","metadata":{"id":"83QJcq4h1Wy2"},"source":["## training loop\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720,"referenced_widgets":["c25623c726e24699b5d499ec46be5f28","d0a493c9e8a6475b9db74e5f49e318a3","0111a295c9af4f168572f9318904f00c","1832a09ce7564b7689f7e41f9364d495","ce433593ab1e43ada8c68a4d0a37df3c","44df2db4e1294413a5b14d87f14bbcce","ddbaf44e9f7748dda478669ff8d6fca6","201e20d78a1241708779c53b63abb034","f107facf0e9a454693c394576bcb2e64","3d9239e4f2cf458db0768fe8ff1dc17a","cbc970dfb1a44ff69bbd803eeff084c2","372b851769ad4c3db387e60f27160ad4","e11fed4e05f144d2b6849b0aebb3528a","e5bc344ea8984ca89b7741ac6e1a267f","c8cb36f4d15c4530897a3fa16e70fc60","88ebd171bb7b4a56989a680f73e1463d","c740f56c4ab641998bb9a67e0585aadf","0c471bb6e83c4b9e982bcec9d4cd784d","cf9503c7f94141aba3b9fcdc2e906c27","f111c27d664a44c4a691b55c01be21fa","0c4b4ce7710746e2869c8d4c2c90d426","7584804e1fbc492b9aaef7c891ca0b38","c274fdc2ed8e4c329c17574f00d2f7a8","1afcd283fb7f4d8786c673dbbf96d592","e3ca60aa77784f788fd82d0b75716360","2c14aeaf416c455fac54aed2c95e41ee","039d13886cac450da8807d6ccd765ed1","2db3c87f274b4c2d803b1569d990d5be","5d0bdc5c93194e7a9e4bc9fbb8674e77","c03c54fba1d44e88aca91ce94cf0bcb5","758e54585bb54f34af4360a65fb975b6","efb6ce93705746808312337202d8f022","5c72a76dbeca45a398a952fb6ffe529f","e2a986df14b44cee995edc39cc4473f8","c6940a0cc04e4e5c87a05d7722bae9af","40bc359cbe814e39ae8b000a44bf8921","7ffe08160b26412abfcdeb237d76e625","ce6c7aeb32b948f6986d3764c8a3601e","c2aef30d03f64844a937686cd11ce17b","ce0f16e3063448be9ccaab3043d26f71","b691fa55c6fc4e35bdac1da78eea66c6","1577d426f138409db1222c9ee1b022e8","cd7625ceed484f6ab3ff9edb0174e526","66c30504d1564e4a8ff18a3545223d14"]},"id":"Z8VJd-zodQyg","executionInfo":{"status":"error","timestamp":1644331773032,"user_tz":-540,"elapsed":34101,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"}},"outputId":"7305db4a-e337-4b49-c46d-84ca7a8026ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["================================================== fold0 training ==================================================\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c25623c726e24699b5d499ec46be5f28","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"372b851769ad4c3db387e60f27160ad4","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type longformer. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c274fdc2ed8e4c329c17574f00d2f7a8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/634M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at SpanBERT/spanbert-large-cased were not used when initializing LongformerModel: ['bert.encoder.layer.12.attention.output.dense.weight', 'bert.encoder.layer.23.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.self.key.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.14.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.16.attention.self.query.weight', 'bert.encoder.layer.12.attention.self.query.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.16.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.dense.bias', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.weight', 'bert.encoder.layer.17.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.12.output.LayerNorm.weight', 'bert.encoder.layer.15.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.16.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.13.attention.output.LayerNorm.bias', 'bert.encoder.layer.18.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.21.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.19.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.22.intermediate.dense.bias', 'bert.encoder.layer.16.attention.self.value.weight', 'bert.encoder.layer.22.attention.self.query.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.13.output.dense.bias', 'bert.encoder.layer.15.attention.self.query.bias', 'bert.encoder.layer.17.attention.self.value.bias', 'bert.encoder.layer.19.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.14.attention.output.LayerNorm.weight', 'bert.encoder.layer.22.output.LayerNorm.weight', 'bert.encoder.layer.16.attention.self.query.bias', 'bert.encoder.layer.16.attention.output.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.18.attention.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.21.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.13.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.23.attention.output.dense.bias', 'bert.encoder.layer.13.output.LayerNorm.bias', 'bert.encoder.layer.15.output.LayerNorm.weight', 'bert.encoder.layer.23.attention.self.value.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.13.attention.output.LayerNorm.weight', 'bert.encoder.layer.22.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.20.attention.self.value.bias', 'bert.encoder.layer.18.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.self.value.bias', 'bert.encoder.layer.19.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.21.output.dense.weight', 'bert.encoder.layer.14.attention.self.key.weight', 'bert.encoder.layer.18.attention.output.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.18.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.14.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.23.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.23.attention.output.LayerNorm.weight', 'bert.encoder.layer.12.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.12.attention.output.LayerNorm.bias', 'bert.encoder.layer.23.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.13.attention.self.query.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.22.attention.output.dense.weight', 'bert.encoder.layer.17.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.18.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.14.attention.output.dense.weight', 'bert.encoder.layer.17.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.20.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.21.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.12.output.LayerNorm.bias', 'bert.encoder.layer.21.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.16.intermediate.dense.bias', 'bert.encoder.layer.17.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.14.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.self.query.weight', 'bert.encoder.layer.15.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.19.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.20.attention.output.dense.bias', 'bert.encoder.layer.17.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.self.query.bias', 'bert.encoder.layer.15.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.15.output.LayerNorm.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.23.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.23.output.dense.weight', 'bert.encoder.layer.22.output.dense.bias', 'bert.encoder.layer.12.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.13.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.20.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.19.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.20.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.14.intermediate.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.18.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.23.output.LayerNorm.bias', 'bert.encoder.layer.12.output.dense.bias', 'bert.encoder.layer.22.attention.self.value.weight', 'bert.encoder.layer.15.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.16.output.dense.bias', 'bert.encoder.layer.22.attention.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.bias', 'bert.encoder.layer.12.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.14.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.13.intermediate.dense.weight', 'bert.encoder.layer.16.attention.self.key.weight', 'bert.encoder.layer.23.attention.self.query.bias', 'bert.encoder.layer.13.attention.self.query.weight', 'bert.encoder.layer.14.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.14.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.17.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.14.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.22.attention.output.dense.bias', 'bert.encoder.layer.13.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.self.query.weight', 'bert.encoder.layer.20.attention.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.20.output.LayerNorm.weight', 'bert.encoder.layer.18.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.16.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.self.key.weight', 'bert.encoder.layer.12.intermediate.dense.bias', 'bert.encoder.layer.20.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.15.attention.output.LayerNorm.weight', 'bert.encoder.layer.15.attention.output.dense.weight', 'bert.encoder.layer.21.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.20.attention.self.key.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.19.attention.self.query.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.22.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.20.attention.self.query.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.14.attention.output.dense.bias', 'bert.encoder.layer.16.output.LayerNorm.bias', 'bert.encoder.layer.16.intermediate.dense.weight', 'bert.encoder.layer.20.output.LayerNorm.bias', 'bert.encoder.layer.18.attention.self.key.bias', 'bert.encoder.layer.21.attention.output.dense.bias', 'bert.encoder.layer.15.attention.self.key.weight', 'bert.encoder.layer.13.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.23.attention.self.key.weight', 'bert.encoder.layer.14.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.17.attention.output.dense.weight', 'bert.encoder.layer.23.intermediate.dense.weight', 'bert.encoder.layer.19.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.20.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.15.attention.self.value.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.23.attention.output.dense.weight', 'bert.encoder.layer.17.attention.self.query.weight', 'bert.encoder.layer.19.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.19.attention.self.value.weight', 'bert.encoder.layer.23.attention.self.value.bias', 'bert.encoder.layer.12.attention.self.value.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.21.attention.output.LayerNorm.bias', 'bert.encoder.layer.21.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.21.attention.self.query.weight', 'bert.encoder.layer.14.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.20.attention.self.value.weight', 'bert.encoder.layer.21.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.self.value.bias', 'bert.encoder.layer.15.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.17.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.18.attention.self.query.weight', 'bert.encoder.layer.14.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.dense.weight', 'bert.encoder.layer.22.intermediate.dense.weight', 'bert.encoder.layer.13.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.21.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.17.output.dense.weight', 'bert.encoder.layer.20.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.12.attention.output.dense.bias', 'bert.encoder.layer.15.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.17.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.17.output.dense.bias', 'bert.encoder.layer.12.attention.self.query.bias', 'bert.encoder.layer.16.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.23.attention.self.query.weight', 'bert.encoder.layer.12.attention.self.key.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.15.attention.self.key.bias', 'bert.encoder.layer.13.output.dense.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.16.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.17.attention.self.key.weight', 'bert.encoder.layer.14.attention.self.value.weight', 'bert.encoder.layer.19.output.LayerNorm.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.22.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.16.attention.self.key.bias', 'bert.encoder.layer.20.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.17.attention.self.query.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.18.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.23.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.12.attention.self.value.bias', 'bert.encoder.layer.18.attention.self.value.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.18.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.19.attention.output.dense.bias', 'bert.encoder.layer.19.attention.output.LayerNorm.bias', 'bert.encoder.layer.13.attention.output.dense.bias', 'bert.encoder.layer.16.attention.self.value.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.22.output.dense.weight', 'bert.encoder.layer.22.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.17.attention.self.value.weight', 'bert.encoder.layer.20.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.21.intermediate.dense.weight', 'bert.encoder.layer.19.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.15.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.bias']\n","- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LongformerModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.16.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.14.attention.self.key_global.bias', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.10.attention.self.value_global.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.22.attention.self.key_global.weight', 'encoder.layer.21.attention.self.query_global.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.5.attention.self.query_global.bias', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.21.attention.self.key_global.bias', 'encoder.layer.22.attention.self.value_global.weight', 'encoder.layer.21.attention.self.value_global.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.2.attention.self.query_global.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.22.attention.self.query_global.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.1.attention.self.key_global.bias', 'encoder.layer.15.attention.self.value_global.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.self.value_global.weight', 'encoder.layer.9.attention.self.query_global.bias', 'encoder.layer.3.attention.self.value_global.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.self.value_global.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.6.attention.self.query_global.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key_global.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.attention.self.value_global.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.23.attention.self.value_global.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.23.attention.self.query_global.weight', 'encoder.layer.16.attention.self.query_global.weight', 'encoder.layer.1.attention.self.value_global.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.19.attention.self.key_global.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.self.value_global.bias', 'encoder.layer.15.attention.self.value_global.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.self.value_global.weight', 'encoder.layer.7.attention.self.query_global.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.15.attention.self.key_global.bias', 'encoder.layer.14.attention.self.value_global.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.12.output.dense.weight', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.4.attention.self.key_global.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.11.attention.self.key_global.bias', 'encoder.layer.16.attention.self.value_global.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.22.attention.self.key_global.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.key_global.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.12.attention.self.key_global.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.8.attention.self.query_global.bias', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key_global.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.23.attention.self.value_global.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.16.attention.self.key_global.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.5.attention.self.key_global.bias', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.8.attention.self.key_global.bias', 'encoder.layer.13.attention.self.key_global.weight', 'encoder.layer.14.attention.self.query_global.bias', 'encoder.layer.17.attention.self.query_global.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.8.attention.self.key_global.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.18.attention.self.key_global.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.9.attention.self.value_global.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.11.attention.self.key_global.weight', 'encoder.layer.17.attention.self.query_global.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.19.attention.self.value_global.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.self.query_global.weight', 'encoder.layer.15.attention.self.query_global.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query_global.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.12.attention.self.value_global.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value_global.weight', 'encoder.layer.5.attention.self.value_global.bias', 'encoder.layer.10.attention.self.query_global.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.6.attention.self.key_global.bias', 'encoder.layer.19.attention.self.query_global.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.22.attention.self.query_global.weight', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.0.attention.self.query_global.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.self.value_global.bias', 'encoder.layer.14.attention.self.query_global.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.6.attention.self.value_global.bias', 'encoder.layer.11.attention.self.query_global.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.self.key_global.weight', 'encoder.layer.19.attention.self.value_global.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.15.attention.self.query_global.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.18.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.15.attention.self.key_global.weight', 'encoder.layer.16.attention.self.key.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.self.query_global.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.13.attention.self.query_global.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.13.attention.self.value_global.bias', 'encoder.layer.16.attention.self.value_global.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.7.attention.self.query_global.weight', 'encoder.layer.12.attention.self.value_global.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.18.attention.self.query_global.bias', 'encoder.layer.14.attention.self.value_global.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.self.key_global.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.17.attention.self.value_global.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.9.attention.self.key_global.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.18.attention.self.key_global.weight', 'encoder.layer.20.attention.self.query_global.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.12.attention.self.key_global.weight', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.21.attention.self.value_global.weight', 'encoder.layer.3.attention.self.query_global.weight', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.19.attention.self.key_global.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.11.attention.self.query_global.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.21.output.dense.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.18.attention.self.value_global.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.20.attention.self.key_global.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.17.attention.self.key_global.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value_global.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.attention.self.value_global.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.21.attention.self.query_global.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.17.attention.self.value_global.weight', 'encoder.layer.3.attention.self.key_global.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.17.attention.self.key_global.bias', 'encoder.layer.6.attention.self.value_global.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.11.attention.self.value_global.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.21.output.dense.weight', 'encoder.layer.3.attention.self.key_global.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.16.attention.self.query_global.bias', 'encoder.layer.14.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.16.attention.self.key_global.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.20.attention.self.query_global.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query_global.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.2.attention.self.value_global.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.20.attention.self.key_global.bias', 'encoder.layer.20.attention.self.value_global.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value_global.weight', 'encoder.layer.10.attention.self.value_global.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.attention.self.value_global.weight', 'encoder.layer.1.attention.self.query_global.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.20.attention.self.value_global.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query_global.weight', 'encoder.layer.21.attention.self.key_global.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.2.attention.self.query_global.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.22.attention.self.value_global.bias', 'encoder.layer.0.attention.self.value_global.bias', 'encoder.layer.10.attention.self.query_global.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.attention.self.query_global.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key_global.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.9.attention.self.value_global.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query_global.bias', 'encoder.layer.3.attention.self.query_global.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.17.output.dense.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.14.attention.self.key_global.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key_global.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.12.attention.self.query_global.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.23.attention.self.query_global.bias', 'encoder.layer.2.attention.self.key_global.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.18.attention.self.value_global.weight', 'encoder.layer.0.attention.self.query_global.bias', 'encoder.layer.5.attention.self.key_global.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.18.attention.self.query_global.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.key_global.weight', 'encoder.layer.13.attention.self.value_global.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key_global.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.23.attention.self.key_global.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'pooler.dense.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.13.attention.self.query_global.bias', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query_global.weight', 'encoder.layer.7.attention.self.key_global.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.10.attention.self.key_global.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.8.attention.self.value_global.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.0.attention.self.key_global.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.12.attention.self.query_global.bias', 'encoder.layer.13.attention.self.key_global.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2a986df14b44cee995edc39cc4473f8","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1560 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["torch.Size([8, 1024]) torch.Size([8, 1024])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-a8898a8328c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtrain_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# train lossの保存\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-d9eb4ba941a0>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, dl_train, optimizer, epoch, criterion)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mraw_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-8d3a310e8a72>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, mask)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1700\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m         )\n\u001b[1;32m   1704\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         \u001b[0mis_index_masked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0mis_index_global_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0mis_global_attn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."]}],"source":["start_time = time.time()\n","\n","oof = pd.DataFrame()\n","for i_fold in range(Config.n_fold):\n","    print('='*50, f'fold{i_fold} training', '='*50)\n","    tokenizer = AutoTokenizer.from_pretrained(Config.model_name, add_prefix_space = True)\n","    model = FeedbackModel()\n","    model = model.to(device)\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=Config.lr)\n","    \n","    df_train = alltrain_texts[alltrain_texts['fold'] != i_fold].reset_index(drop = True)\n","    ds_train = FeedbackPrizeDataset(df_train, tokenizer, Config.max_length, True)\n","    df_val = alltrain_texts[alltrain_texts['fold'] == i_fold].reset_index(drop = True)\n","    val_idlist = df_val['id'].unique().tolist()\n","    df_val_eval = df_alltrain.query('id==@val_idlist').reset_index(drop=True)\n","    ds_val = FeedbackPrizeDataset(df_val, tokenizer, Config.max_length, True)\n","    dl_train = DataLoader(ds_train, batch_size=Config.train_batch_size, shuffle=True, num_workers=2, pin_memory=True)\n","    dl_val = DataLoader(ds_val, batch_size=Config.valid_batch_size, shuffle=False, num_workers=2, pin_memory=True)\n","\n","    best_val_loss = np.inf\n","    criterion = nn.CrossEntropyLoss()\n","\n","    train_loss_history = []\n","    valid_loss_history = []\n","\n","    for epoch in range(1, Config.n_epoch + 1):\n","        train_loss = train_fn(model, dl_train, optimizer, epoch, criterion) # train\n","        train_loss_history.append(train_loss) # train lossの保存\n","\n","        valid_loss, _oof = valid_fn(model, df_val, df_val_eval, dl_val, epoch, criterion) # validation\n","        valid_loss_history.append(valid_loss) # valid lossの保存\n","        if valid_loss < best_val_loss:\n","            best_val_loss = valid_loss\n","            _oof_fold_best = _oof\n","            _oof_fold_best['fold'] = i_fold\n","            model_filename = f'{Config.model_dir}/{Config.model_savename}_{i_fold}.bin'\n","            torch.save(model.state_dict(), model_filename)\n","            print(f'{model_filename} saved')\n","    \n","    # lossの描画\n","    fig, ax = plt.subplots(1,1, figsize=(10,6))\n","    sns.lineplot(data=train_loss_history, label='train loss')\n","    sns.lineplot(data=valid_loss_history, label='valid loss')\n","    ax.set_title(f'loss history: fold{i_fold}')\n","    plt.legend();\n","\n","    oof = pd.concat([oof, _oof_fold_best])\n","    del df_train, ds_train, df_val, val_idlist, df_val_eval, ds_val, dl_train, dl_val, tokenizer, model, optimizer\n","    gc.collect()\n","\n","print(f'{time.time() - start_time:.1f}s')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDt2P8nznAb0","executionInfo":{"status":"aborted","timestamp":1644329884788,"user_tz":-540,"elapsed":6,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"}}},"outputs":[],"source":["oof.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JpRQZ3rCIYTm","executionInfo":{"status":"aborted","timestamp":1644329434908,"user_tz":-540,"elapsed":12,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"}}},"outputs":[],"source":["oof.to_csv(f'{Config.output_dir}/oof_{Config.name}.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJZUUpbeIlpb","executionInfo":{"status":"aborted","timestamp":1644329434909,"user_tz":-540,"elapsed":12,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"}}},"outputs":[],"source":["pd.read_csv(f'{Config.output_dir}/oof_{Config.name}.csv').head()"]},{"cell_type":"markdown","metadata":{"id":"OnI_IEod28V0"},"source":["## cv score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qoFvFhk9obhS","executionInfo":{"status":"aborted","timestamp":1644329434909,"user_tz":-540,"elapsed":12,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"}}},"outputs":[],"source":["if Config.is_debug:\n","    idlist = alltrain_texts['id'].unique().tolist()\n","    df_train = df_alltrain.query('id==@idlist')\n","else:\n","    df_train = df_alltrain.copy()\n","print(f'overall cv score: {oof_score(df_train, oof)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O0bR-tBiBo6g","executionInfo":{"status":"aborted","timestamp":1644329434910,"user_tz":-540,"elapsed":13,"user":{"displayName":"武井柊悟","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00119739872577936431"}}},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"nb008.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c25623c726e24699b5d499ec46be5f28":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0a493c9e8a6475b9db74e5f49e318a3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0111a295c9af4f168572f9318904f00c","IPY_MODEL_1832a09ce7564b7689f7e41f9364d495","IPY_MODEL_ce433593ab1e43ada8c68a4d0a37df3c"]}},"d0a493c9e8a6475b9db74e5f49e318a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0111a295c9af4f168572f9318904f00c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_44df2db4e1294413a5b14d87f14bbcce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ddbaf44e9f7748dda478669ff8d6fca6"}},"1832a09ce7564b7689f7e41f9364d495":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_201e20d78a1241708779c53b63abb034","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":414,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":414,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f107facf0e9a454693c394576bcb2e64"}},"ce433593ab1e43ada8c68a4d0a37df3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3d9239e4f2cf458db0768fe8ff1dc17a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 414/414 [00:00&lt;00:00, 14.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cbc970dfb1a44ff69bbd803eeff084c2"}},"44df2db4e1294413a5b14d87f14bbcce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ddbaf44e9f7748dda478669ff8d6fca6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"201e20d78a1241708779c53b63abb034":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f107facf0e9a454693c394576bcb2e64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3d9239e4f2cf458db0768fe8ff1dc17a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cbc970dfb1a44ff69bbd803eeff084c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"372b851769ad4c3db387e60f27160ad4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e11fed4e05f144d2b6849b0aebb3528a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e5bc344ea8984ca89b7741ac6e1a267f","IPY_MODEL_c8cb36f4d15c4530897a3fa16e70fc60","IPY_MODEL_88ebd171bb7b4a56989a680f73e1463d"]}},"e11fed4e05f144d2b6849b0aebb3528a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5bc344ea8984ca89b7741ac6e1a267f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c740f56c4ab641998bb9a67e0585aadf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c471bb6e83c4b9e982bcec9d4cd784d"}},"c8cb36f4d15c4530897a3fa16e70fc60":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cf9503c7f94141aba3b9fcdc2e906c27","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f111c27d664a44c4a691b55c01be21fa"}},"88ebd171bb7b4a56989a680f73e1463d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0c4b4ce7710746e2869c8d4c2c90d426","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 208k/208k [00:00&lt;00:00, 628kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7584804e1fbc492b9aaef7c891ca0b38"}},"c740f56c4ab641998bb9a67e0585aadf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0c471bb6e83c4b9e982bcec9d4cd784d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf9503c7f94141aba3b9fcdc2e906c27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f111c27d664a44c4a691b55c01be21fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c4b4ce7710746e2869c8d4c2c90d426":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7584804e1fbc492b9aaef7c891ca0b38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c274fdc2ed8e4c329c17574f00d2f7a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1afcd283fb7f4d8786c673dbbf96d592","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e3ca60aa77784f788fd82d0b75716360","IPY_MODEL_2c14aeaf416c455fac54aed2c95e41ee","IPY_MODEL_039d13886cac450da8807d6ccd765ed1"]}},"1afcd283fb7f4d8786c673dbbf96d592":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e3ca60aa77784f788fd82d0b75716360":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2db3c87f274b4c2d803b1569d990d5be","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5d0bdc5c93194e7a9e4bc9fbb8674e77"}},"2c14aeaf416c455fac54aed2c95e41ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c03c54fba1d44e88aca91ce94cf0bcb5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":665132540,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":665132540,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_758e54585bb54f34af4360a65fb975b6"}},"039d13886cac450da8807d6ccd765ed1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_efb6ce93705746808312337202d8f022","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 634M/634M [00:17&lt;00:00, 38.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5c72a76dbeca45a398a952fb6ffe529f"}},"2db3c87f274b4c2d803b1569d990d5be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5d0bdc5c93194e7a9e4bc9fbb8674e77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c03c54fba1d44e88aca91ce94cf0bcb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"758e54585bb54f34af4360a65fb975b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"efb6ce93705746808312337202d8f022":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5c72a76dbeca45a398a952fb6ffe529f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e2a986df14b44cee995edc39cc4473f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c6940a0cc04e4e5c87a05d7722bae9af","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_40bc359cbe814e39ae8b000a44bf8921","IPY_MODEL_7ffe08160b26412abfcdeb237d76e625","IPY_MODEL_ce6c7aeb32b948f6986d3764c8a3601e"]}},"c6940a0cc04e4e5c87a05d7722bae9af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"40bc359cbe814e39ae8b000a44bf8921":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c2aef30d03f64844a937686cd11ce17b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"  0%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce0f16e3063448be9ccaab3043d26f71"}},"7ffe08160b26412abfcdeb237d76e625":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b691fa55c6fc4e35bdac1da78eea66c6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":1560,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1577d426f138409db1222c9ee1b022e8"}},"ce6c7aeb32b948f6986d3764c8a3601e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cd7625ceed484f6ab3ff9edb0174e526","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/1560 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_66c30504d1564e4a8ff18a3545223d14"}},"c2aef30d03f64844a937686cd11ce17b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce0f16e3063448be9ccaab3043d26f71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b691fa55c6fc4e35bdac1da78eea66c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1577d426f138409db1222c9ee1b022e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cd7625ceed484f6ab3ff9edb0174e526":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"66c30504d1564e4a8ff18a3545223d14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}